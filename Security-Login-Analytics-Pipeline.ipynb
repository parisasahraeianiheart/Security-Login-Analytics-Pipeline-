{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "ad77b138-5fda-4705-a8a4-9796f11af4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "try:\n",
    "    PROJECT_ROOT = Path(__file__).resolve().parents[1]\n",
    "except NameError:\n",
    "    PROJECT_ROOT = Path.cwd()\n",
    "\n",
    "DATA_DIR = PROJECT_ROOT / \"data\"\n",
    "RAW_DIR = DATA_DIR / \"raw\"\n",
    "STAGING_DIR = DATA_DIR / \"staging\"\n",
    "WAREHOUSE_DIR = PROJECT_ROOT / \"warehouse\"\n",
    "DB_PATH = WAREHOUSE_DIR / \"security.duckdb\"\n",
    "\n",
    "RAW_DIR.mkdir(parents=True, exist_ok=True)\n",
    "STAGING_DIR.mkdir(parents=True, exist_ok=True)\n",
    "WAREHOUSE_DIR.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "2e760e26-3967-4b68-8fb2-714848fefff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "def get_logger(name: str = \"pipeline\") -> logging.Logger:\n",
    "    logger = logging.getLogger(name)\n",
    "    if not logger.handlers:\n",
    "        logger.setLevel(logging.INFO)\n",
    "        handler = logging.StreamHandler()\n",
    "        fmt = logging.Formatter(\"%(asctime)s | %(levelname)s | %(message)s\")\n",
    "        handler.setFormatter(fmt)\n",
    "        logger.addHandler(handler)\n",
    "    return logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "1e181492-c41d-4278-b9bc-81a083b38e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def assert_not_null(df: pd.DataFrame, cols: list[str]) -> None:\n",
    "    missing = df[cols].isna().mean()\n",
    "    bad = missing[missing > 0].sort_values(ascending=False)\n",
    "    if not bad.empty:\n",
    "        raise ValueError(f\"NOT NULL check failed:\\n{bad}\")\n",
    "\n",
    "def assert_unique(df: pd.DataFrame, cols: list[str]) -> None:\n",
    "    dupes = df.duplicated(subset=cols).sum()\n",
    "    if dupes > 0:\n",
    "        raise ValueError(f\"UNIQUE check failed on {cols}. Duplicates: {dupes}\")\n",
    "\n",
    "def assert_accepted_values(df: pd.DataFrame, col: str, allowed: set) -> None:\n",
    "    bad = set(df[col].dropna().unique()) - allowed\n",
    "    if bad:\n",
    "        raise ValueError(f\"Accepted values check failed for {col}. Bad: {bad}\")\n",
    "\n",
    "def assert_row_count(df: pd.DataFrame, min_rows: int) -> None:\n",
    "    if len(df) < min_rows:\n",
    "        raise ValueError(f\"Row count check failed. Got {len(df)}, expected >= {min_rows}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "6b9bf88a-0ee8-4334-af18-a4fb2780750d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/sahra\n",
      "src path = ['/Users/sahra/src']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# 1) remove already-imported src\n",
    "sys.modules.pop(\"src\", None)\n",
    "\n",
    "# 2) point to your ACTUAL project root (edit this!)\n",
    "PROJECT_ROOT = Path(\"/Users/sahra\").resolve()\n",
    "print(PROJECT_ROOT)\n",
    "# 3) put it at the front so it wins imports\n",
    "sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "# 4) re-import and confirm\n",
    "import src\n",
    "print(\"src path =\", list(getattr(src, \"__path__\", [])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "abab46cb-7edf-46c5-9473-446ad10f6fed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-29 21:16:53,994 | INFO | Wrote raw files to: /Users/sahra/data/raw\n",
      "2026-01-29 21:16:53,995 | INFO | Files: user_profiles.csv, ip_reputation.csv, auth_events.csv\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "from src.config import RAW_DIR\n",
    "from src.utils_logging import get_logger\n",
    "\n",
    "logger = get_logger(\"generate_data\")\n",
    "\n",
    "def main(n_users: int = 2000, n_events: int = 20000, seed: int = 42) -> None:\n",
    "    rng = np.random.default_rng(seed)\n",
    "\n",
    "    # -------------------------\n",
    "    # 1) USERS TABLE\n",
    "    # -------------------------\n",
    "    users = pd.DataFrame({\n",
    "        \"user_id\": np.arange(1, n_users + 1),\n",
    "        \"country\": rng.choice([\"US\", \"CA\", \"UK\", \"DE\", \"FR\", \"IN\"], size=n_users,\n",
    "                              p=[0.45, 0.10, 0.10, 0.10, 0.10, 0.15]),\n",
    "        \"plan\": rng.choice([\"free\", \"pro\", \"enterprise\"], size=n_users,\n",
    "                           p=[0.60, 0.30, 0.10]),\n",
    "        \"account_age_days\": rng.integers(1, 2500, size=n_users),\n",
    "    })\n",
    "\n",
    "    # -------------------------\n",
    "    # 2) IP REPUTATION TABLE (MAKE IPS UNIQUE!)\n",
    "    # -------------------------\n",
    "    n_ips = 5000\n",
    "\n",
    "    # Generate unique IPs safely by using a set\n",
    "    ips = set()\n",
    "    while len(ips) < n_ips:\n",
    "        ips.add(f\"10.{rng.integers(0,256)}.{rng.integers(0,256)}.{rng.integers(0,256)}\")\n",
    "    ips = list(ips)\n",
    "\n",
    "    ip_rep = pd.DataFrame({\n",
    "        \"ip\": ips,\n",
    "        \"ip_risk\": np.clip(rng.normal(0.25, 0.2, size=n_ips), 0, 1),\n",
    "        \"is_known_bad\": (rng.random(n_ips) < 0.03).astype(int)\n",
    "    })\n",
    "\n",
    "    # -------------------------\n",
    "    # 3) AUTH EVENTS TABLE\n",
    "    # -------------------------\n",
    "    start = datetime(2025, 1, 1)\n",
    "    timestamps = [start + timedelta(minutes=int(x)) for x in rng.integers(0, 60 * 24 * 60, size=n_events)]  # 60 days\n",
    "\n",
    "    events = pd.DataFrame({\n",
    "        \"event_id\": np.arange(1, n_events + 1),\n",
    "        \"user_id\": rng.integers(1, n_users + 1, size=n_events),\n",
    "        \"timestamp\": timestamps,\n",
    "        \"device_type\": rng.choice([\"mobile\", \"desktop\", \"tablet\"], size=n_events, p=[0.5, 0.4, 0.1]),\n",
    "        \"browser\": rng.choice([\"chrome\", \"firefox\", \"safari\", \"edge\"], size=n_events, p=[0.55, 0.15, 0.2, 0.1]),\n",
    "        \"is_vpn\": (rng.random(n_events) < 0.2).astype(int),\n",
    "        \"failed_logins\": rng.poisson(1.2, size=n_events),\n",
    "        \"session_duration\": rng.exponential(120, size=n_events),\n",
    "        \"bytes_sent\": rng.lognormal(10, 1.0, size=n_events),\n",
    "        \"bytes_received\": rng.lognormal(10.2, 1.1, size=n_events),\n",
    "        \"ip\": rng.choice(ip_rep[\"ip\"].values, size=n_events),\n",
    "        \"success\": (rng.random(n_events) > 0.15).astype(int)\n",
    "    })\n",
    "\n",
    "    # -------------------------\n",
    "    # 4) CREATE LABEL WITH REALISTIC SIGNAL\n",
    "    #    (JOIN USERS + IP_REP WITHOUT ROW EXPLOSION)\n",
    "    # -------------------------\n",
    "    tmp = events.merge(users[[\"user_id\", \"account_age_days\", \"plan\"]], on=\"user_id\", how=\"left\")\n",
    "    tmp = tmp.merge(ip_rep[[\"ip\", \"ip_risk\", \"is_known_bad\"]], on=\"ip\", how=\"left\")\n",
    "\n",
    "    # Safety check: joins must not change row count\n",
    "    if len(tmp) != len(events):\n",
    "        raise ValueError(f\"Join exploded rows: tmp={len(tmp)} events={len(events)}. Check join keys uniqueness.\")\n",
    "\n",
    "    prob = (\n",
    "        0.25 * tmp[\"ip_risk\"].fillna(0) +\n",
    "        0.25 * (tmp[\"failed_logins\"] > 2).astype(int) +\n",
    "        0.15 * tmp[\"is_vpn\"] +\n",
    "        0.15 * (tmp[\"account_age_days\"] < 30).astype(int) +\n",
    "        0.10 * (tmp[\"plan\"] == \"free\").astype(int) +\n",
    "        0.20 * tmp[\"is_known_bad\"].fillna(0)\n",
    "    )\n",
    "\n",
    "    noise = rng.normal(0, 0.08, size=len(tmp))\n",
    "    events[\"label\"] = ((prob + noise) > 0.55).astype(int).to_numpy()\n",
    "\n",
    "    # -------------------------\n",
    "    # 5) INJECT MISSING VALUES (REALISTIC DATA ISSUES)\n",
    "    # -------------------------\n",
    "    for col in [\"session_duration\", \"bytes_sent\"]:\n",
    "        idx = rng.choice(events.index, size=int(0.02 * n_events), replace=False)\n",
    "        events.loc[idx, col] = np.nan\n",
    "\n",
    "    idx = rng.choice(users.index, size=int(0.02 * n_users), replace=False)\n",
    "    users.loc[idx, \"country\"] = np.nan\n",
    "\n",
    "    # -------------------------\n",
    "    # 6) WRITE RAW FILES\n",
    "    # -------------------------\n",
    "    users.to_csv(RAW_DIR / \"user_profiles.csv\", index=False)\n",
    "    ip_rep.to_csv(RAW_DIR / \"ip_reputation.csv\", index=False)\n",
    "    events.to_csv(RAW_DIR / \"auth_events.csv\", index=False)\n",
    "\n",
    "    logger.info(f\"Wrote raw files to: {RAW_DIR}\")\n",
    "    logger.info(\"Files: user_profiles.csv, ip_reputation.csv, auth_events.csv\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "6d92f508-96fc-434b-b911-76afc82aee88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-29 21:16:54,807 | INFO | Extracted raw datasets\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from src.config import RAW_DIR\n",
    "from src.utils_quality import assert_row_count\n",
    "from src.utils_logging import get_logger\n",
    "\n",
    "logger = get_logger()\n",
    "\n",
    "def extract() -> dict[str, pd.DataFrame]:\n",
    "    users = pd.read_csv(RAW_DIR / \"user_profiles.csv\")\n",
    "    ip_rep = pd.read_csv(RAW_DIR / \"ip_reputation.csv\")\n",
    "    events = pd.read_csv(RAW_DIR / \"auth_events.csv\")\n",
    "\n",
    "    assert_row_count(users, 100)\n",
    "    assert_row_count(ip_rep, 100)\n",
    "    assert_row_count(events, 1000)\n",
    "\n",
    "    logger.info(\"Extracted raw datasets\")\n",
    "    return {\"users\": users, \"ip_rep\": ip_rep, \"events\": events}\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    extract()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "d79bff74-7097-475c-844f-538557e114ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-29 21:16:55,352 | INFO | Extracted raw datasets\n",
      "2026-01-29 21:16:55,414 | INFO | Wrote staging parquet to /Users/sahra/data/staging\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from src.config import STAGING_DIR\n",
    "from src.utils_logging import get_logger\n",
    "from src.utils_quality import assert_accepted_values, assert_unique\n",
    "\n",
    "logger = get_logger()\n",
    "\n",
    "def transform(dfs: dict[str, pd.DataFrame]) -> dict[str, pd.DataFrame]:\n",
    "    users = dfs[\"users\"].copy()\n",
    "    ip_rep = dfs[\"ip_rep\"].copy()\n",
    "    events = dfs[\"events\"].copy()\n",
    "\n",
    "    # types\n",
    "    events[\"timestamp\"] = pd.to_datetime(events[\"timestamp\"], errors=\"coerce\")\n",
    "\n",
    "    # basic cleaning\n",
    "    events = events.drop_duplicates(subset=[\"event_id\"])\n",
    "    users = users.drop_duplicates(subset=[\"user_id\"])\n",
    "    ip_rep = ip_rep.drop_duplicates(subset=[\"ip\"])\n",
    "\n",
    "    # quality: accepted values\n",
    "    assert_accepted_values(events, \"device_type\", {\"mobile\",\"desktop\",\"tablet\"})\n",
    "    assert_accepted_values(events, \"browser\", {\"chrome\",\"firefox\",\"safari\",\"edge\"})\n",
    "\n",
    "    # missing handling (simple, interview-ready)\n",
    "    num_cols = [\"session_duration\",\"bytes_sent\",\"bytes_received\",\"failed_logins\"]\n",
    "    for c in num_cols:\n",
    "        events[c] = pd.to_numeric(events[c], errors=\"coerce\")\n",
    "        events[c] = events[c].fillna(events[c].median())\n",
    "\n",
    "    events[\"is_vpn\"] = events[\"is_vpn\"].fillna(0).astype(int)\n",
    "    events[\"success\"] = events[\"success\"].fillna(0).astype(int)\n",
    "    events[\"label\"] = events[\"label\"].fillna(0).astype(int)\n",
    "\n",
    "    # join keys checks\n",
    "    assert_unique(users, [\"user_id\"])\n",
    "    assert_unique(ip_rep, [\"ip\"])\n",
    "\n",
    "    # write staging (parquet is better than csv)\n",
    "    users.to_parquet(STAGING_DIR / \"stg_users.parquet\", index=False)\n",
    "    ip_rep.to_parquet(STAGING_DIR / \"stg_ip_rep.parquet\", index=False)\n",
    "    events.to_parquet(STAGING_DIR / \"stg_events.parquet\", index=False)\n",
    "\n",
    "    logger.info(f\"Wrote staging parquet to {STAGING_DIR}\")\n",
    "    return {\"stg_users\": users, \"stg_ip_rep\": ip_rep, \"stg_events\": events}\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # quick local test\n",
    "    from src.etl_extract import extract\n",
    "    transform(extract())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "3e3bb38b-6b33-45e3-ba02-09a256e647e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: duckdb in /opt/anaconda3/lib/python3.12/site-packages (1.4.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install duckdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "7404d137-dada-4635-a686-04c6e3b03bff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-29 21:16:57,832 | INFO | Loaded staging tables into /Users/sahra/warehouse/security.duckdb\n"
     ]
    }
   ],
   "source": [
    "import duckdb\n",
    "from src.config import DB_PATH, STAGING_DIR\n",
    "from src.utils_logging import get_logger\n",
    "\n",
    "logger = get_logger()\n",
    "\n",
    "def load_to_duckdb() -> None:\n",
    "    con = duckdb.connect(str(DB_PATH))\n",
    "\n",
    "    con.execute(\"CREATE SCHEMA IF NOT EXISTS raw;\")\n",
    "    con.execute(\"CREATE SCHEMA IF NOT EXISTS staging;\")\n",
    "    con.execute(\"CREATE SCHEMA IF NOT EXISTS analytics;\")\n",
    "\n",
    "    # idempotent loads: replace tables\n",
    "    con.execute(\"DROP TABLE IF EXISTS staging.stg_users;\")\n",
    "    con.execute(\"DROP TABLE IF EXISTS staging.stg_ip_rep;\")\n",
    "    con.execute(\"DROP TABLE IF EXISTS staging.stg_events;\")\n",
    "\n",
    "    con.execute(f\"\"\"\n",
    "        CREATE TABLE staging.stg_users AS\n",
    "        SELECT * FROM read_parquet('{STAGING_DIR / \"stg_users.parquet\"}');\n",
    "    \"\"\")\n",
    "    con.execute(f\"\"\"\n",
    "        CREATE TABLE staging.stg_ip_rep AS\n",
    "        SELECT * FROM read_parquet('{STAGING_DIR / \"stg_ip_rep.parquet\"}');\n",
    "    \"\"\")\n",
    "    con.execute(f\"\"\"\n",
    "        CREATE TABLE staging.stg_events AS\n",
    "        SELECT * FROM read_parquet('{STAGING_DIR / \"stg_events.parquet\"}');\n",
    "    \"\"\")\n",
    "\n",
    "    con.close()\n",
    "    logger.info(f\"Loaded staging tables into {DB_PATH}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    load_to_duckdb()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "441f7861-55e3-4d05-be9c-826ef97594c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-29 21:16:59,116 | INFO | Built analytics models: dims, fact, marts\n"
     ]
    }
   ],
   "source": [
    "import duckdb\n",
    "from src.config import DB_PATH\n",
    "from src.utils_logging import get_logger\n",
    "\n",
    "logger = get_logger()\n",
    "\n",
    "def build_models() -> None:\n",
    "    con = duckdb.connect(str(DB_PATH))\n",
    "    con.execute(\"CREATE SCHEMA IF NOT EXISTS analytics;\")\n",
    "\n",
    "    # dim_date\n",
    "    con.execute(\"DROP TABLE IF EXISTS analytics.dim_date;\")\n",
    "    con.execute(\"\"\"\n",
    "        CREATE TABLE analytics.dim_date AS\n",
    "        SELECT DISTINCT\n",
    "            CAST(timestamp AS DATE) AS date,\n",
    "            EXTRACT(year FROM timestamp) AS year,\n",
    "            EXTRACT(month FROM timestamp) AS month,\n",
    "            EXTRACT(day FROM timestamp) AS day,\n",
    "            EXTRACT(dow FROM timestamp) AS day_of_week\n",
    "        FROM staging.stg_events;\n",
    "    \"\"\")\n",
    "\n",
    "    # dim_user\n",
    "    con.execute(\"DROP TABLE IF EXISTS analytics.dim_user;\")\n",
    "    con.execute(\"\"\"\n",
    "        CREATE TABLE analytics.dim_user AS\n",
    "        SELECT\n",
    "            user_id,\n",
    "            country,\n",
    "            plan,\n",
    "            account_age_days\n",
    "        FROM staging.stg_users;\n",
    "    \"\"\")\n",
    "\n",
    "    # fact_auth_event\n",
    "    con.execute(\"DROP TABLE IF EXISTS analytics.fact_auth_event;\")\n",
    "    con.execute(\"\"\"\n",
    "        CREATE TABLE analytics.fact_auth_event AS\n",
    "        SELECT\n",
    "            e.event_id,\n",
    "            e.user_id,\n",
    "            CAST(e.timestamp AS DATE) AS date,\n",
    "            e.timestamp,\n",
    "            e.device_type,\n",
    "            e.browser,\n",
    "            e.is_vpn,\n",
    "            e.failed_logins,\n",
    "            e.session_duration,\n",
    "            e.bytes_sent,\n",
    "            e.bytes_received,\n",
    "            e.ip,\n",
    "            r.ip_risk,\n",
    "            r.is_known_bad,\n",
    "            e.success,\n",
    "            e.label\n",
    "        FROM staging.stg_events e\n",
    "        LEFT JOIN staging.stg_ip_rep r USING (ip);\n",
    "    \"\"\")\n",
    "\n",
    "    # mart: daily KPIs\n",
    "    con.execute(\"DROP TABLE IF EXISTS analytics.mart_daily_kpis;\")\n",
    "    con.execute(\"\"\"\n",
    "        CREATE TABLE analytics.mart_daily_kpis AS\n",
    "        SELECT\n",
    "            date,\n",
    "            COUNT(*) AS total_events,\n",
    "            AVG(success) AS success_rate,\n",
    "            AVG(label) AS suspicious_rate,\n",
    "            AVG(ip_risk) AS avg_ip_risk,\n",
    "            AVG(failed_logins) AS avg_failed_logins,\n",
    "            SUM(CASE WHEN is_known_bad=1 THEN 1 ELSE 0 END) AS known_bad_events\n",
    "        FROM analytics.fact_auth_event\n",
    "        GROUP BY 1\n",
    "        ORDER BY 1;\n",
    "    \"\"\")\n",
    "\n",
    "    # mart: risk segments\n",
    "    con.execute(\"DROP TABLE IF EXISTS analytics.mart_risk_segments;\")\n",
    "    con.execute(\"\"\"\n",
    "        CREATE TABLE analytics.mart_risk_segments AS\n",
    "        SELECT\n",
    "            CASE\n",
    "                WHEN ip_risk >= 0.7 OR is_known_bad=1 THEN 'high'\n",
    "                WHEN ip_risk >= 0.4 THEN 'medium'\n",
    "                ELSE 'low'\n",
    "            END AS risk_segment,\n",
    "            COUNT(*) AS events,\n",
    "            AVG(label) AS suspicious_rate,\n",
    "            AVG(success) AS success_rate\n",
    "        FROM analytics.fact_auth_event\n",
    "        GROUP BY 1\n",
    "        ORDER BY suspicious_rate DESC;\n",
    "    \"\"\")\n",
    "\n",
    "    con.close()\n",
    "    logger.info(\"Built analytics models: dims, fact, marts\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    build_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "f41d507f-26e7-4cc1-9c8b-b1a6d7e96871",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-29 21:17:00,438 | INFO | Analytics checks passed ✅\n"
     ]
    }
   ],
   "source": [
    "import duckdb\n",
    "from src.config import DB_PATH\n",
    "from src.utils_logging import get_logger\n",
    "\n",
    "logger = get_logger()\n",
    "\n",
    "def run_checks() -> None:\n",
    "    con = duckdb.connect(str(DB_PATH))\n",
    "\n",
    "    # Not null checks\n",
    "    nn = con.execute(\"\"\"\n",
    "        SELECT\n",
    "          SUM(CASE WHEN user_id IS NULL THEN 1 ELSE 0 END) AS null_user_id,\n",
    "          SUM(CASE WHEN event_id IS NULL THEN 1 ELSE 0 END) AS null_event_id\n",
    "        FROM analytics.fact_auth_event;\n",
    "    \"\"\").fetchone()\n",
    "\n",
    "    if nn[0] > 0 or nn[1] > 0:\n",
    "        raise ValueError(f\"NOT NULL failed: {nn}\")\n",
    "\n",
    "    # Unique key checks\n",
    "    dupes = con.execute(\"\"\"\n",
    "        SELECT COUNT(*) FROM (\n",
    "            SELECT event_id, COUNT(*) c\n",
    "            FROM analytics.fact_auth_event\n",
    "            GROUP BY 1\n",
    "            HAVING c > 1\n",
    "        );\n",
    "    \"\"\").fetchone()[0]\n",
    "    if dupes > 0:\n",
    "        raise ValueError(f\"UNIQUE failed: duplicate event_id groups={dupes}\")\n",
    "\n",
    "    # Accepted values checks\n",
    "    bad_device = con.execute(\"\"\"\n",
    "        SELECT COUNT(*) FROM analytics.fact_auth_event\n",
    "        WHERE device_type NOT IN ('mobile','desktop','tablet');\n",
    "    \"\"\").fetchone()[0]\n",
    "    if bad_device > 0:\n",
    "        raise ValueError(f\"Accepted values failed for device_type: {bad_device}\")\n",
    "\n",
    "    con.close()\n",
    "    logger.info(\"Analytics checks passed ✅\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_checks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "9918dbf2-247e-4ca7-b79c-e2b03f780d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "import duckdb\n",
    "import pandas as pd\n",
    "from src.config import DB_PATH\n",
    "\n",
    "st.set_page_config(page_title=\"Security Login Analytics\", layout=\"wide\")\n",
    "st.title(\"Security Login Analytics Dashboard\")\n",
    "\n",
    "con = duckdb.connect(str(DB_PATH), read_only=True)\n",
    "\n",
    "kpis = con.execute(\"SELECT * FROM analytics.mart_daily_kpis\").df()\n",
    "seg = con.execute(\"SELECT * FROM analytics.mart_risk_segments\").df()\n",
    "\n",
    "st.subheader(\"Daily KPIs\")\n",
    "st.line_chart(kpis.set_index(\"date\")[[\"total_events\",\"suspicious_rate\",\"success_rate\"]])\n",
    "\n",
    "st.subheader(\"Risk Segments\")\n",
    "st.dataframe(seg)\n",
    "\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed6a723-c526-460d-9c57-ecc491538ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To run: PYTHONPATH=/Users/.../src/dashboard_app.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
